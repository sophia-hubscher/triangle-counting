\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\global \c@tocdepth 2\relax }
\citation{lovasz_large_2012}
\citation{su_effect_2016}
\citation{ye_commensurate_2005}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{4}{section.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Graph with triangles formed between vertices (A, B, C), (B, C, D) and (B, D, E).}}{4}{figure.caption.1}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:triangles}{{1}{4}{Graph with triangles formed between vertices (A, B, C), (B, C, D) and (B, D, E)}{figure.caption.1}{}}
\newlabel{fig:triangles@cref}{{[figure][1][]1}{[1][4][]4}}
\citation{al_hasan_triangle_2018}
\citation{schank_finding_2005}
\citation{strassen_gaussian_1969}
\citation{tsourakakis_doulion_2009,seshadhri_triadic_2013,tsourakakis_fast_2008}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Dense Graph with many edges relative to the number of nodes.}}{5}{figure.caption.2}\protected@file@percent }
\newlabel{fig:dense_graph}{{2}{5}{Dense Graph with many edges relative to the number of nodes}{figure.caption.2}{}}
\newlabel{fig:dense_graph@cref}{{[figure][2][]2}{[1][5][]5}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Sparse Graph with few edges relative to the number of nodes.}}{5}{figure.caption.2}\protected@file@percent }
\newlabel{fig:sparse_graph}{{3}{5}{Sparse Graph with few edges relative to the number of nodes}{figure.caption.2}{}}
\newlabel{fig:sparse_graph@cref}{{[figure][3][]3}{[1][5][]5}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Notation}{7}{section.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces List of notation used.}}{7}{table.caption.3}\protected@file@percent }
\newlabel{tab:notation}{{1}{7}{List of notation used}{table.caption.3}{}}
\newlabel{tab:notation@cref}{{[table][1][]1}{[1][7][]7}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Background}{9}{section.3}\protected@file@percent }
\newlabel{sec:background}{{3}{9}{Background}{section.3}{}}
\newlabel{sec:background@cref}{{[section][3][]3}{[1][9][]9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Types of Graphs}{9}{subsection.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Graph Motifs}{9}{subsection.3.2}\protected@file@percent }
\citation{al_hasan_triangle_2018}
\citation{schank_finding_2005}
\citation{schank_finding_2005}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces The complete graph \( K_4 \) on four vertices.}}{10}{figure.caption.4}\protected@file@percent }
\newlabel{fig:k4}{{4}{10}{The complete graph \( K_4 \) on four vertices}{figure.caption.4}{}}
\newlabel{fig:k4@cref}{{[figure][4][]4}{[1][9][]10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Methods for Triangle Counting}{10}{subsection.3.3}\protected@file@percent }
\newlabel{sec:exact-counting}{{3.3}{10}{Methods for Triangle Counting}{subsection.3.3}{}}
\newlabel{sec:exact-counting@cref}{{[subsection][3][3]3.3}{[1][10][]10}}
\citation{tsourakakis_doulion_2009}
\citation{arifuzzaman_parallel_2012}
\citation{seshadhri_triadic_2013}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.1}Sampling Methods}{11}{subsubsection.3.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Edge Sampling}{11}{subsubsection.3.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Wedge Sampling}{11}{subsubsection.3.3.1}\protected@file@percent }
\citation{beum_method_1950}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Wedge formed by vertices $u$, $v$, and $w$. Nodes $v$ and $w$ may or may not be connected.}}{12}{figure.caption.5}\protected@file@percent }
\newlabel{fig:wedge_diagram}{{5}{12}{Wedge formed by vertices $u$, $v$, and $w$. Nodes $v$ and $w$ may or may not be connected}{figure.caption.5}{}}
\newlabel{fig:wedge_diagram@cref}{{[figure][5][]5}{[1][11][]12}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.2}Linear Algebraic Methods}{12}{subsubsection.3.3.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Graph representation of vertices A, B, C, and D.}}{12}{figure.caption.6}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Adjacency matrix corresponding to the graph.}}{12}{figure.caption.6}\protected@file@percent }
\citation{strassen_gaussian_1969}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Six different ways to arrive at a length-three path in a triangle.}}{13}{figure.caption.7}\protected@file@percent }
\newlabel{fig:triangle-traversal}{{8}{13}{Six different ways to arrive at a length-three path in a triangle}{figure.caption.7}{}}
\newlabel{fig:triangle-traversal@cref}{{[figure][8][]8}{[1][13][]13}}
\@writefile{toc}{\contentsline {paragraph}{Strassen's Algorithm}{13}{figure.caption.7}\protected@file@percent }
\citation{williams_new_2023}
\citation{tsourakakis_fast_2008}
\@writefile{toc}{\contentsline {paragraph}{EigenTriangle}{14}{figure.caption.7}\protected@file@percent }
\citation{cullum_lanczos_2002}
\citation{avron_counting_2010}
\citation{hutchinson_stochastic_1990}
\citation{avron_counting_2010}
\citation{avron_counting_2010}
\@writefile{toc}{\contentsline {paragraph}{TraceTriangle}{15}{figure.caption.7}\protected@file@percent }
\citation{roughgarden_algorithms_2020}
\citation{duan_linear-time_2014}
\citation{dinitz_faster_2021}
\citation{boldrin_fast_2024}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}General Algorithmic Strategies}{16}{subsection.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.1}Learning-Augmented Algorithms}{16}{subsubsection.3.4.1}\protected@file@percent }
\citation{prescott_monte_1965}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.2}Variance Reduction}{17}{subsubsection.3.4.2}\protected@file@percent }
\newlabel{sec:variance-reduction-background}{{3.4.2}{17}{Variance Reduction}{subsubsection.3.4.2}{}}
\newlabel{sec:variance-reduction-background@cref}{{[subsubsection][2][3,4]3.4.2}{[1][17][]17}}
\citation{lovasz_large_2012}
\citation{motwani_randomized_1995}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.3}Importance Sampling}{19}{subsubsection.3.4.3}\protected@file@percent }
\newlabel{sec:importance-sampling-background}{{3.4.3}{19}{Importance Sampling}{subsubsection.3.4.3}{}}
\newlabel{sec:importance-sampling-background@cref}{{[subsubsection][3][3,4]3.4.3}{[1][19][]19}}
\newlabel{eq:D}{{1}{20}{Importance Sampling}{equation.3.1}{}}
\newlabel{eq:D@cref}{{[equation][1][]1}{[1][19][]20}}
\newlabel{eq:pi}{{2}{20}{Importance Sampling}{equation.3.2}{}}
\newlabel{eq:pi@cref}{{[equation][2][]2}{[1][20][]20}}
\citation{hagberg_exploring_2008}
\citation{leskovec_snap_2017}
\@writefile{toc}{\contentsline {section}{\numberline {4}Methods}{21}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Implementation}{21}{subsection.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Datasets}{21}{subsection.4.2}\protected@file@percent }
\citation{albert_statistical_2002}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Sampling Methods Evaluated}{22}{subsection.4.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Degree vs. triangle count for the Facebook dataset. The slope $\alpha \approx 1.97$.}}{24}{figure.caption.8}\protected@file@percent }
\newlabel{fig:degree_vs_tri_fb}{{9}{24}{Degree vs. triangle count for the Facebook dataset. The slope $\alpha \approx 1.97$}{figure.caption.8}{}}
\newlabel{fig:degree_vs_tri_fb@cref}{{[figure][9][]9}{[1][24][]24}}
\newlabel{fig:degree_vs_tri_croc}{{10a}{25}{Wikipedia}{figure.caption.9}{}}
\newlabel{fig:degree_vs_tri_croc@cref}{{[subfigure][1][10]10a}{[1][24][]25}}
\newlabel{sub@fig:degree_vs_tri_croc}{{a}{25}{Wikipedia}{figure.caption.9}{}}
\newlabel{sub@fig:degree_vs_tri_croc@cref}{{[subfigure][1][10]10a}{[1][24][]25}}
\newlabel{fig:degree_vs_tri_GrQc}{{10b}{25}{Collaboration}{figure.caption.9}{}}
\newlabel{fig:degree_vs_tri_GrQc@cref}{{[subfigure][2][10]10b}{[1][24][]25}}
\newlabel{sub@fig:degree_vs_tri_GrQc}{{b}{25}{Collaboration}{figure.caption.9}{}}
\newlabel{sub@fig:degree_vs_tri_GrQc@cref}{{[subfigure][2][10]10b}{[1][24][]25}}
\newlabel{fig:degree_vs_tri_ba}{{10c}{25}{Barabási–Albert}{figure.caption.9}{}}
\newlabel{fig:degree_vs_tri_ba@cref}{{[subfigure][3][10]10c}{[1][24][]25}}
\newlabel{sub@fig:degree_vs_tri_ba}{{c}{25}{Barabási–Albert}{figure.caption.9}{}}
\newlabel{sub@fig:degree_vs_tri_ba@cref}{{[subfigure][3][10]10c}{[1][24][]25}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Degree vs. triangle count across datasets. For the Wikipedia dataset $\alpha \approx 1.57$, for the collaboration network $\alpha \approx 1.84$, and for the Barabási–Albert graph $\alpha \approx 2.02$.}}{25}{figure.caption.9}\protected@file@percent }
\newlabel{fig:degree_vs_tri_others}{{10}{25}{Degree vs. triangle count across datasets. For the Wikipedia dataset $\alpha \approx 1.57$, for the collaboration network $\alpha \approx 1.84$, and for the Barabási–Albert graph $\alpha \approx 2.02$}{figure.caption.9}{}}
\newlabel{fig:degree_vs_tri_others@cref}{{[figure][10][]10}{[1][24][]25}}
\newlabel{variance-reduction-algo}{{4.3}{25}{Sampling Methods Evaluated}{figure.caption.9}{}}
\newlabel{variance-reduction-algo@cref}{{[subsection][3][4]4.3}{[1][25][]25}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Evaluation Metrics}{28}{subsection.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5}Additional Explorations}{28}{subsection.4.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.5.1}4-Clique Variant}{29}{subsubsection.4.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.5.2}Simulated Method Tests}{29}{subsubsection.4.5.2}\protected@file@percent }
\newlabel{sec:methods-simulated-test}{{4.5.2}{29}{Simulated Method Tests}{subsubsection.4.5.2}{}}
\newlabel{sec:methods-simulated-test@cref}{{[subsubsection][2][4,5]4.5.2}{[1][29][]29}}
\newlabel{fig:uniform_noise}{{11a}{30}{Uniform}{figure.caption.10}{}}
\newlabel{fig:uniform_noise@cref}{{[subfigure][1][11]11a}{[1][30][]30}}
\newlabel{sub@fig:uniform_noise}{{a}{30}{Uniform}{figure.caption.10}{}}
\newlabel{sub@fig:uniform_noise@cref}{{[subfigure][1][11]11a}{[1][30][]30}}
\newlabel{fig:multiplicative_noise}{{11b}{30}{Multiplicative}{figure.caption.10}{}}
\newlabel{fig:multiplicative_noise@cref}{{[subfigure][2][11]11b}{[1][30][]30}}
\newlabel{sub@fig:multiplicative_noise}{{b}{30}{Multiplicative}{figure.caption.10}{}}
\newlabel{sub@fig:multiplicative_noise@cref}{{[subfigure][2][11]11b}{[1][30][]30}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Triangle Count vs Noise for Uniform and Multiplicative Noise}}{30}{figure.caption.10}\protected@file@percent }
\newlabel{fig:noise_shapes}{{11}{30}{Triangle Count vs Noise for Uniform and Multiplicative Noise}{figure.caption.10}{}}
\newlabel{fig:noise_shapes@cref}{{[figure][11][]11}{[1][30][]30}}
\newlabel{fig:tri_vs_noise_fb}{{12a}{31}{Facebook}{figure.caption.11}{}}
\newlabel{fig:tri_vs_noise_fb@cref}{{[subfigure][1][12]12a}{[1][30][]31}}
\newlabel{sub@fig:tri_vs_noise_fb}{{a}{31}{Facebook}{figure.caption.11}{}}
\newlabel{sub@fig:tri_vs_noise_fb@cref}{{[subfigure][1][12]12a}{[1][30][]31}}
\newlabel{fig:tri_vs_noise_croc}{{12b}{31}{Wikipedia}{figure.caption.11}{}}
\newlabel{fig:tri_vs_noise_croc@cref}{{[subfigure][2][12]12b}{[1][30][]31}}
\newlabel{sub@fig:tri_vs_noise_croc}{{b}{31}{Wikipedia}{figure.caption.11}{}}
\newlabel{sub@fig:tri_vs_noise_croc@cref}{{[subfigure][2][12]12b}{[1][30][]31}}
\newlabel{fig:tri_vs_noise_GrQc}{{12c}{31}{Collaboration}{figure.caption.11}{}}
\newlabel{fig:tri_vs_noise_GrQc@cref}{{[subfigure][3][12]12c}{[1][30][]31}}
\newlabel{sub@fig:tri_vs_noise_GrQc}{{c}{31}{Collaboration}{figure.caption.11}{}}
\newlabel{sub@fig:tri_vs_noise_GrQc@cref}{{[subfigure][3][12]12c}{[1][30][]31}}
\newlabel{fig:tri_vs_noise_ba}{{12d}{31}{Barabási–Albert}{figure.caption.11}{}}
\newlabel{fig:tri_vs_noise_ba@cref}{{[subfigure][4][12]12d}{[1][30][]31}}
\newlabel{sub@fig:tri_vs_noise_ba}{{d}{31}{Barabási–Albert}{figure.caption.11}{}}
\newlabel{sub@fig:tri_vs_noise_ba@cref}{{[subfigure][4][12]12d}{[1][30][]31}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Triangle counts vs. noise across all datasets. Nodes with higher triangle counts often have higher noise.}}{31}{figure.caption.11}\protected@file@percent }
\newlabel{fig:tri_vs_noise}{{12}{31}{Triangle counts vs. noise across all datasets. Nodes with higher triangle counts often have higher noise}{figure.caption.11}{}}
\newlabel{fig:tri_vs_noise@cref}{{[figure][12][]12}{[1][30][]31}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Results and Discussion}{32}{section.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Triangle Counting}{32}{subsection.5.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Percent Errors for Different Methods on Various Datasets (Sample Size = 100, $\alpha = 2$)}}{32}{table.caption.12}\protected@file@percent }
\newlabel{tab:percent_error_100}{{2}{32}{Percent Errors for Different Methods on Various Datasets (Sample Size = 100, $\alpha = 2$)}{table.caption.12}{}}
\newlabel{tab:percent_error_100@cref}{{[table][2][]2}{[1][32][]32}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Percent Errors for Different Methods on Various Datasets (Sample Size = 4000, $\alpha = 2$)}}{32}{table.caption.13}\protected@file@percent }
\newlabel{tab:percent_error_4000}{{3}{32}{Percent Errors for Different Methods on Various Datasets (Sample Size = 4000, $\alpha = 2$)}{table.caption.13}{}}
\newlabel{tab:percent_error_4000@cref}{{[table][3][]3}{[1][32][]32}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Percent Error vs. Sample Size for Facebook Dataset. Importance sampling and the hybrid approach perform best, with the hybrid method performing slightly better.}}{33}{figure.caption.14}\protected@file@percent }
\newlabel{fig:fb_sample_size}{{13}{33}{Percent Error vs. Sample Size for Facebook Dataset. Importance sampling and the hybrid approach perform best, with the hybrid method performing slightly better}{figure.caption.14}{}}
\newlabel{fig:fb_sample_size@cref}{{[figure][13][]13}{[1][32][]33}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Percent Error vs. Sample Size for Collaboration Network Dataset. Importance sampling and the hybrid approach perform best.}}{33}{figure.caption.15}\protected@file@percent }
\newlabel{fig:grqc_sample_size}{{14}{33}{Percent Error vs. Sample Size for Collaboration Network Dataset. Importance sampling and the hybrid approach perform best}{figure.caption.15}{}}
\newlabel{fig:grqc_sample_size@cref}{{[figure][14][]14}{[1][33][]33}}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces Percent Error vs. Sample Size for Crocodile Wikipedia Dataset. The hybrid method performs best.}}{33}{figure.caption.16}\protected@file@percent }
\newlabel{fig:croc_sample_size}{{15}{33}{Percent Error vs. Sample Size for Crocodile Wikipedia Dataset. The hybrid method performs best}{figure.caption.16}{}}
\newlabel{fig:croc_sample_size@cref}{{[figure][15][]15}{[1][33][]33}}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces Percent Error vs. Sample Size for Synthetic Barabási–Albert Dataset. Importance sampling, variance reduction, and the hybrid method perform best, with importance sampling and the hybrid method performing best.}}{34}{figure.caption.17}\protected@file@percent }
\newlabel{fig:ba_sample_size}{{16}{34}{Percent Error vs. Sample Size for Synthetic Barabási–Albert Dataset. Importance sampling, variance reduction, and the hybrid method perform best, with importance sampling and the hybrid method performing best}{figure.caption.17}{}}
\newlabel{fig:ba_sample_size@cref}{{[figure][16][]16}{[1][33][]34}}
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces Percent Error vs. Runtime for Facebook Dataset. Uniform sampling, importance sampling, and the hybrid method have similar errors by runtime, but importance sampling and the hybrid method achieve the lowest errors overall.}}{34}{figure.caption.18}\protected@file@percent }
\newlabel{fig:fb_runtime}{{17}{34}{Percent Error vs. Runtime for Facebook Dataset. Uniform sampling, importance sampling, and the hybrid method have similar errors by runtime, but importance sampling and the hybrid method achieve the lowest errors overall}{figure.caption.18}{}}
\newlabel{fig:fb_runtime@cref}{{[figure][17][]17}{[1][34][]34}}
\@writefile{lof}{\contentsline {figure}{\numberline {18}{\ignorespaces Percent Error vs. Runtime for Collaboration Network Dataset. Importance sampling outperforms other methods.}}{35}{figure.caption.19}\protected@file@percent }
\newlabel{fig:grqc_runtime}{{18}{35}{Percent Error vs. Runtime for Collaboration Network Dataset. Importance sampling outperforms other methods}{figure.caption.19}{}}
\newlabel{fig:grqc_runtime@cref}{{[figure][18][]18}{[1][34][]35}}
\@writefile{lof}{\contentsline {figure}{\numberline {19}{\ignorespaces Percent Error vs. Runtime for Crocodile Wikipedia Dataset. Uniform sampling has a far shorter runtime than the other methods with comparable accuracy for large samples.}}{35}{figure.caption.20}\protected@file@percent }
\newlabel{fig:croc_runtime}{{19}{35}{Percent Error vs. Runtime for Crocodile Wikipedia Dataset. Uniform sampling has a far shorter runtime than the other methods with comparable accuracy for large samples}{figure.caption.20}{}}
\newlabel{fig:croc_runtime@cref}{{[figure][19][]19}{[1][35][]35}}
\@writefile{lof}{\contentsline {figure}{\numberline {20}{\ignorespaces Percent Error vs. Runtime for Synthetic Barabási–Albert Dataset. The hybrid approach yields the lowest overall error, but for shorter runtimes, importance sampling performs best.}}{35}{figure.caption.21}\protected@file@percent }
\newlabel{fig:ba_runtime}{{20}{35}{Percent Error vs. Runtime for Synthetic Barabási–Albert Dataset. The hybrid approach yields the lowest overall error, but for shorter runtimes, importance sampling performs best}{figure.caption.21}{}}
\newlabel{fig:ba_runtime@cref}{{[figure][20][]20}{[1][35][]35}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.1}$\alpha $ Tradeoff}{36}{subsubsection.5.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {21}{\ignorespaces Average Duration - Importance Sampling. Runtime increases with sample size and power.}}{36}{figure.caption.22}\protected@file@percent }
\newlabel{fig:avg_duration_importance}{{21}{36}{Average Duration - Importance Sampling. Runtime increases with sample size and power}{figure.caption.22}{}}
\newlabel{fig:avg_duration_importance@cref}{{[figure][21][]21}{[1][36][]36}}
\@writefile{lof}{\contentsline {figure}{\numberline {22}{\ignorespaces Average Duration - Hybrid Method. Runtime increases with sample size and power.}}{36}{figure.caption.23}\protected@file@percent }
\newlabel{fig:avg_duration_variance_importance}{{22}{36}{Average Duration - Hybrid Method. Runtime increases with sample size and power}{figure.caption.23}{}}
\newlabel{fig:avg_duration_variance_importance@cref}{{[figure][22][]22}{[1][36][]36}}
\@writefile{lof}{\contentsline {figure}{\numberline {23}{\ignorespaces Percent Error - Importance Sampling. Percent error decreases as sample size increases. Powers closest to ~1.97 perform best.}}{37}{figure.caption.24}\protected@file@percent }
\newlabel{fig:percent_error_importance}{{23}{37}{Percent Error - Importance Sampling. Percent error decreases as sample size increases. Powers closest to ~1.97 perform best}{figure.caption.24}{}}
\newlabel{fig:percent_error_importance@cref}{{[figure][23][]23}{[1][36][]37}}
\@writefile{lof}{\contentsline {figure}{\numberline {24}{\ignorespaces Percent Error - Hybrid Method. Percent error decreases as sample size increases. Powers closest to ~1.97 perform best.}}{37}{figure.caption.25}\protected@file@percent }
\newlabel{fig:percent_error_variance_importance}{{24}{37}{Percent Error - Hybrid Method. Percent error decreases as sample size increases. Powers closest to ~1.97 perform best}{figure.caption.25}{}}
\newlabel{fig:percent_error_variance_importance@cref}{{[figure][24][]24}{[1][37][]37}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}4-Clique Counting}{38}{subsection.5.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {25}{\ignorespaces Comparison of percent error versus runtime for different methods in 4-clique counting. Importance sampling with higher powers outperforms variance reduction.}}{38}{figure.caption.26}\protected@file@percent }
\newlabel{fig:4_clique_percent_error_runtime_comparison}{{25}{38}{Comparison of percent error versus runtime for different methods in 4-clique counting. Importance sampling with higher powers outperforms variance reduction}{figure.caption.26}{}}
\newlabel{fig:4_clique_percent_error_runtime_comparison@cref}{{[figure][25][]25}{[1][38][]38}}
\@writefile{lof}{\contentsline {figure}{\numberline {26}{\ignorespaces Comparison of percent error versus sample size for different 4-clique counting methods. Importance sampling with higher powers outperforms variance reduction.}}{39}{figure.caption.27}\protected@file@percent }
\newlabel{fig:4_clique_percent_error_sample_size_comparison}{{26}{39}{Comparison of percent error versus sample size for different 4-clique counting methods. Importance sampling with higher powers outperforms variance reduction}{figure.caption.27}{}}
\newlabel{fig:4_clique_percent_error_sample_size_comparison@cref}{{[figure][26][]26}{[1][38][]39}}
\@writefile{lof}{\contentsline {figure}{\numberline {27}{\ignorespaces Average duration of the importance sampling method across powers for 4-clique counting. Runtime increases with sample size and power.}}{39}{figure.caption.28}\protected@file@percent }
\newlabel{fig:4_clique_avg_duration_importance_sampling}{{27}{39}{Average duration of the importance sampling method across powers for 4-clique counting. Runtime increases with sample size and power}{figure.caption.28}{}}
\newlabel{fig:4_clique_avg_duration_importance_sampling@cref}{{[figure][27][]27}{[1][39][]39}}
\@writefile{lof}{\contentsline {figure}{\numberline {28}{\ignorespaces Average error of the importance sampling method across powers for 4-clique counting. Average error decreases as sample size increases. The power of 1.5 performs best.}}{40}{figure.caption.29}\protected@file@percent }
\newlabel{fig:4_clique_avg_error_importance_sampling}{{28}{40}{Average error of the importance sampling method across powers for 4-clique counting. Average error decreases as sample size increases. The power of 1.5 performs best}{figure.caption.29}{}}
\newlabel{fig:4_clique_avg_error_importance_sampling@cref}{{[figure][28][]28}{[1][39][]40}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Understanding the Benefits of Importance Sampling vs. Variance Reduction}{40}{subsection.5.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.3.1}Uniform Noise Relationship}{40}{subsubsection.5.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {29}{\ignorespaces Sample size vs. average error with uniform noise (scale = 10). Variance reduction outperforms importance sampling.}}{41}{figure.caption.30}\protected@file@percent }
\newlabel{fig:uniform_noise_10}{{29}{41}{Sample size vs. average error with uniform noise (scale = 10). Variance reduction outperforms importance sampling}{figure.caption.30}{}}
\newlabel{fig:uniform_noise_10@cref}{{[figure][29][]29}{[1][41][]41}}
\@writefile{lof}{\contentsline {figure}{\numberline {30}{\ignorespaces Sample size vs. average error with uniform noise (scale = 100). Variance reduction outperforms importance sampling.}}{41}{figure.caption.31}\protected@file@percent }
\newlabel{fig:uniform_noise_100}{{30}{41}{Sample size vs. average error with uniform noise (scale = 100). Variance reduction outperforms importance sampling}{figure.caption.31}{}}
\newlabel{fig:uniform_noise_100@cref}{{[figure][30][]30}{[1][41][]41}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.3.2}Multiplicative Noise Relationship}{42}{subsubsection.5.3.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {31}{\ignorespaces Sample size vs. average error with multiplicative noise (scale = 0.01). Importance sampling outperforms variance reduction.}}{42}{figure.caption.32}\protected@file@percent }
\newlabel{fig:multiplicative_noise_001}{{31}{42}{Sample size vs. average error with multiplicative noise (scale = 0.01). Importance sampling outperforms variance reduction}{figure.caption.32}{}}
\newlabel{fig:multiplicative_noise_001@cref}{{[figure][31][]31}{[1][42][]42}}
\@writefile{lof}{\contentsline {figure}{\numberline {32}{\ignorespaces Sample size vs. average error with multiplicative noise (scale = 0.1). Importance sampling outperforms variance reduction.}}{43}{figure.caption.33}\protected@file@percent }
\newlabel{fig:multiplicative_noise_01}{{32}{43}{Sample size vs. average error with multiplicative noise (scale = 0.1). Importance sampling outperforms variance reduction}{figure.caption.33}{}}
\newlabel{fig:multiplicative_noise_01@cref}{{[figure][32][]32}{[1][42][]43}}
\newlabel{theorem:variance-comparison}{{1}{43}{Multiplicative Noise Relationship}{theorem.1}{}}
\newlabel{theorem:variance-comparison@cref}{{[theorem][1][]1}{[1][43][]43}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Conclusion}{44}{section.6}\protected@file@percent }
\bibstyle{plain}
\bibdata{thesis_bib}
\bibcite{al_hasan_triangle_2018}{1}
\bibcite{albert_statistical_2002}{2}
\bibcite{arifuzzaman_parallel_2012}{3}
\bibcite{avron_counting_2010}{4}
\bibcite{beum_method_1950}{5}
\bibcite{boldrin_fast_2024}{6}
\bibcite{cullum_lanczos_2002}{7}
\bibcite{dinitz_faster_2021}{8}
\bibcite{duan_linear-time_2014}{9}
\bibcite{hagberg_exploring_2008}{10}
\bibcite{hutchinson_stochastic_1990}{11}
\bibcite{leskovec_snap_2017}{12}
\bibcite{lovasz_large_2012}{13}
\bibcite{roughgarden_algorithms_2020}{14}
\bibcite{motwani_randomized_1995}{15}
\bibcite{prescott_monte_1965}{16}
\bibcite{schank_finding_2005}{17}
\bibcite{seshadhri_triadic_2013}{18}
\bibcite{strassen_gaussian_1969}{19}
\bibcite{su_effect_2016}{20}
\bibcite{tsourakakis_fast_2008}{21}
\bibcite{tsourakakis_doulion_2009}{22}
\bibcite{williams_new_2023}{23}
\bibcite{ye_commensurate_2005}{24}
\@writefile{toc}{\contentsline {section}{\numberline {7}Appendix}{48}{section.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1}Links to Code and Extended Results}{48}{subsection.7.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2}Theoretical Analysis of Variances}{49}{subsection.7.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.2.1}Uniform Sampling}{49}{subsubsection.7.2.1}\protected@file@percent }
\newlabel{eq:Var_Delta_US}{{3}{50}{Uniform Sampling}{equation.7.3}{}}
\newlabel{eq:Var_Delta_US@cref}{{[equation][3][]3}{[1][50][]50}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.2.2}Importance Sampling}{51}{subsubsection.7.2.2}\protected@file@percent }
\newlabel{eq:Var_Delta_IS}{{4}{53}{Importance Sampling}{equation.7.4}{}}
\newlabel{eq:Var_Delta_IS@cref}{{[equation][4][]4}{[1][53][]53}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.2.3}Variance Reduction}{53}{subsubsection.7.2.3}\protected@file@percent }
\newlabel{sec:variance-reduction-analysis}{{7.2.3}{53}{Variance Reduction}{subsubsection.7.2.3}{}}
\newlabel{sec:variance-reduction-analysis@cref}{{[subsubsection][3][7,2]7.2.3}{[1][53][]53}}
\@writefile{toc}{\contentsline {paragraph}{Variance Reduction with Uniform Noise}{54}{subsubsection.7.2.3}\protected@file@percent }
\newlabel{eq:Var_Delta_VRu}{{5}{55}{Variance Reduction with Uniform Noise}{equation.7.5}{}}
\newlabel{eq:Var_Delta_VRu@cref}{{[equation][5][]5}{[1][55][]55}}
\@writefile{toc}{\contentsline {paragraph}{Variance Reduction with Multiplicative Noise}{56}{equation.7.5}\protected@file@percent }
\newlabel{eq:Var_Delta_VRm}{{6}{57}{Variance Reduction with Multiplicative Noise}{equation.7.6}{}}
\newlabel{eq:Var_Delta_VRm@cref}{{[equation][6][]6}{[1][56][]57}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.2.4}Comparing Variance Reduction and Importance Sampling}{57}{subsubsection.7.2.4}\protected@file@percent }
\newlabel{appendix:variance-comparison}{{7.2.4}{57}{Comparing Variance Reduction and Importance Sampling}{subsubsection.7.2.4}{}}
\newlabel{appendix:variance-comparison@cref}{{[subsubsection][4][7,2]7.2.4}{[1][57][]57}}
\gdef \@abspage@last{58}
