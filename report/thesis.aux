\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{lovasz_large_2012}
\citation{su_effect_2016}
\citation{ye_commensurate_2005}
\citation{al_hasan_triangle_2018}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{5}{section.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Graph with triangles formed between vertices (A, B, C), (B, C, D) and (B, D, E).}}{5}{figure.caption.1}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:triangles}{{1}{5}{Graph with triangles formed between vertices (A, B, C), (B, C, D) and (B, D, E)}{figure.caption.1}{}}
\citation{strassen_gaussian_1969}
\citation{tsourakakis_doulion_2009,seshadhri_triadic_2013,tsourakakis_fast_2008}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Dense Graph with many edges relative to the number of nodes.}}{6}{figure.caption.2}\protected@file@percent }
\newlabel{fig:dense_graph}{{2}{6}{Dense Graph with many edges relative to the number of nodes}{figure.caption.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Sparse Graph with few edges relative to the number of nodes.}}{6}{figure.caption.2}\protected@file@percent }
\newlabel{fig:sparse_graph}{{3}{6}{Sparse Graph with few edges relative to the number of nodes}{figure.caption.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Notation}{7}{section.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces List of notation used.}}{7}{table.caption.3}\protected@file@percent }
\newlabel{tab:notation}{{1}{7}{List of notation used}{table.caption.3}{}}
\citation{al_hasan_triangle_2018}
\@writefile{toc}{\contentsline {section}{\numberline {3}Background}{8}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Types of Graphs}{8}{subsection.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Graph Motifs}{8}{subsection.3.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces The complete graph \( K_4 \) on four vertices.}}{8}{figure.caption.4}\protected@file@percent }
\newlabel{fig:k4}{{4}{8}{The complete graph \( K_4 \) on four vertices}{figure.caption.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Methods for Triangle Counting}{8}{subsection.3.3}\protected@file@percent }
\citation{tsourakakis_doulion_2009}
\citation{arifuzzaman_parallel_2012}
\citation{seshadhri_triadic_2013}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.1}Sampling Methods}{9}{subsubsection.3.3.1}\protected@file@percent }
\citation{beum_method_1950}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Wedge formed by vertices $u$, $v$, and $w$. Nodes $v$ and $w$ may or may not be connected.}}{10}{figure.caption.5}\protected@file@percent }
\newlabel{fig:wedge_diagram}{{5}{10}{Wedge formed by vertices $u$, $v$, and $w$. Nodes $v$ and $w$ may or may not be connected}{figure.caption.5}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.2}Linear Algebraic Methods}{10}{subsubsection.3.3.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Graph representation of vertices A, B, C, and D.}}{10}{figure.caption.6}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Adjacency matrix corresponding to the graph.}}{10}{figure.caption.6}\protected@file@percent }
\citation{strassen_gaussian_1969}
\citation{williams_new_2023}
\citation{tsourakakis_fast_2008}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Six different ways to arrive at a length-three path in a triangle.}}{11}{figure.caption.7}\protected@file@percent }
\newlabel{fig:triangle-traversal}{{8}{11}{Six different ways to arrive at a length-three path in a triangle}{figure.caption.7}{}}
\citation{cullum_lanczos_2002}
\citation{avron_counting_2010}
\citation{hutchinson_stochastic_1990}
\citation{avron_counting_2010}
\citation{prescott_monte_1965}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}General Algorithmic Strategies}{13}{subsection.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.1}Variance Reduction}{13}{subsubsection.3.4.1}\protected@file@percent }
\newlabel{sec:variance-reduction-background}{{3.4.1}{13}{Variance Reduction}{subsubsection.3.4.1}{}}
\citation{lovasz_large_2012}
\citation{motwani_randomized_1995}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.2}Importance Sampling}{14}{subsubsection.3.4.2}\protected@file@percent }
\newlabel{sec:importance-sampling-background}{{3.4.2}{14}{Importance Sampling}{subsubsection.3.4.2}{}}
\citation{roughgarden_algorithms_2020}
\citation{duan_linear-time_2014}
\citation{dinitz_faster_2021}
\citation{boldrin_fast_2024}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.3}Learning-Augmented Algorithms}{15}{subsubsection.3.4.3}\protected@file@percent }
\citation{albert_statistical_2002}
\@writefile{toc}{\contentsline {section}{\numberline {4}Methods}{16}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Implementation}{16}{subsection.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Datasets}{16}{subsection.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Sampling Methods Evaluated}{17}{subsection.4.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Degree vs. triangle count for the Facebook dataset. The slope $\alpha \approx 1.97$.}}{18}{figure.caption.8}\protected@file@percent }
\newlabel{fig:degree_vs_tri_fb}{{9}{18}{Degree vs. triangle count for the Facebook dataset. The slope $\alpha \approx 1.97$}{figure.caption.8}{}}
\newlabel{fig:degree_vs_tri_croc}{{10a}{19}{Wikipedia}{figure.caption.9}{}}
\newlabel{sub@fig:degree_vs_tri_croc}{{a}{19}{Wikipedia}{figure.caption.9}{}}
\newlabel{fig:degree_vs_tri_GrQc}{{10b}{19}{Collaboration}{figure.caption.9}{}}
\newlabel{sub@fig:degree_vs_tri_GrQc}{{b}{19}{Collaboration}{figure.caption.9}{}}
\newlabel{fig:degree_vs_tri_ba}{{10c}{19}{Barabási–Albert}{figure.caption.9}{}}
\newlabel{sub@fig:degree_vs_tri_ba}{{c}{19}{Barabási–Albert}{figure.caption.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Degree vs. triangle count across datasets. For the Wikipedia dataset $\alpha \approx 1.57$, for the collaboration network $\alpha \approx 1.84$, and for the Barabási–Albert graph $\alpha \approx 2.02$.}}{19}{figure.caption.9}\protected@file@percent }
\newlabel{fig:degree_vs_tri_others}{{10}{19}{Degree vs. triangle count across datasets. For the Wikipedia dataset $\alpha \approx 1.57$, for the collaboration network $\alpha \approx 1.84$, and for the Barabási–Albert graph $\alpha \approx 2.02$}{figure.caption.9}{}}
\newlabel{variance-reduction-algo}{{4.3}{19}{Sampling Methods Evaluated}{figure.caption.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Evaluation Metrics}{21}{subsection.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5}Additional Explorations}{21}{subsection.4.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.5.1}4-Clique Variant}{21}{subsubsection.4.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.5.2}Simulated Method Tests}{21}{subsubsection.4.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5}Results}{23}{section.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Triangle Counting}{23}{subsection.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.1}Average Percent Error By Method}{23}{subsubsection.5.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Percent Error - Uniform Sampling. Percent error decreases as sample size increases.}}{23}{figure.caption.10}\protected@file@percent }
\newlabel{fig:percent_error_uniform}{{11}{23}{Percent Error - Uniform Sampling. Percent error decreases as sample size increases}{figure.caption.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Percent Error - Importance Sampling. Percent error decreases as sample size increases. Powers closest to ~1.97 perform best.}}{23}{figure.caption.11}\protected@file@percent }
\newlabel{fig:percent_error_importance}{{12}{23}{Percent Error - Importance Sampling. Percent error decreases as sample size increases. Powers closest to ~1.97 perform best}{figure.caption.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Percent Error - Variance Reduction. Percent error decreases as sample size increases once the sample size is large enough.}}{24}{figure.caption.12}\protected@file@percent }
\newlabel{fig:percent_error_variance}{{13}{24}{Percent Error - Variance Reduction. Percent error decreases as sample size increases once the sample size is large enough}{figure.caption.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Percent Error - Variance Reduction + Importance Sampling. Percent error decreases as sample size increases. Powers closest to ~1.97 perform best.}}{24}{figure.caption.13}\protected@file@percent }
\newlabel{fig:percent_error_variance_importance}{{14}{24}{Percent Error - Variance Reduction + Importance Sampling. Percent error decreases as sample size increases. Powers closest to ~1.97 perform best}{figure.caption.13}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.2}Average Duration (Runtime) By Method}{24}{subsubsection.5.1.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces Average Duration - Uniform Sampling. Runtime increases with sample size.}}{25}{figure.caption.14}\protected@file@percent }
\newlabel{fig:avg_duration_uniform}{{15}{25}{Average Duration - Uniform Sampling. Runtime increases with sample size}{figure.caption.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces Average Duration - Importance Sampling. Runtime increases with sample size and power.}}{25}{figure.caption.15}\protected@file@percent }
\newlabel{fig:avg_duration_importance}{{16}{25}{Average Duration - Importance Sampling. Runtime increases with sample size and power}{figure.caption.15}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces Average Duration - Variance Reduction. Runtime increases with sample size.}}{25}{figure.caption.16}\protected@file@percent }
\newlabel{fig:avg_duration_variance}{{17}{25}{Average Duration - Variance Reduction. Runtime increases with sample size}{figure.caption.16}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {18}{\ignorespaces Average Duration - Variance Reduction + Importance Sampling. Runtime increases with sample size and power.}}{26}{figure.caption.17}\protected@file@percent }
\newlabel{fig:avg_duration_variance_importance}{{18}{26}{Average Duration - Variance Reduction + Importance Sampling. Runtime increases with sample size and power}{figure.caption.17}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.3}Accuracy and Runtime Comparisons}{26}{subsubsection.5.1.3}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Percent Errors for Different Methods on Various Datasets (Sample Size = 100, Power = 2)}}{26}{table.caption.18}\protected@file@percent }
\newlabel{tab:percent_error_100}{{2}{26}{Percent Errors for Different Methods on Various Datasets (Sample Size = 100, Power = 2)}{table.caption.18}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Percent Errors for Different Methods on Various Datasets (Sample Size = 4000, Power = 2)}}{26}{table.caption.19}\protected@file@percent }
\newlabel{tab:percent_error_4000}{{3}{26}{Percent Errors for Different Methods on Various Datasets (Sample Size = 4000, Power = 2)}{table.caption.19}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {19}{\ignorespaces Percent Error vs. Runtime for Facebook Dataset. Uniform sampling, importance sampling, and the hybrid method have similar errors by runtime, but importance sampling and the hybrid method achieve the lowest errors overall.}}{27}{figure.caption.20}\protected@file@percent }
\newlabel{fig:fb_runtime}{{19}{27}{Percent Error vs. Runtime for Facebook Dataset. Uniform sampling, importance sampling, and the hybrid method have similar errors by runtime, but importance sampling and the hybrid method achieve the lowest errors overall}{figure.caption.20}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {20}{\ignorespaces Percent Error vs. Runtime for Collaboration Network Dataset. Importance sampling outperforms other methods.}}{27}{figure.caption.21}\protected@file@percent }
\newlabel{fig:grqc_runtime}{{20}{27}{Percent Error vs. Runtime for Collaboration Network Dataset. Importance sampling outperforms other methods}{figure.caption.21}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {21}{\ignorespaces Percent Error vs. Runtime for Crocodile Wikipedia Dataset. Uniform sampling has a far shorter runtime than the other methods with comparable accuracy for large samples.}}{28}{figure.caption.22}\protected@file@percent }
\newlabel{fig:croc_runtime}{{21}{28}{Percent Error vs. Runtime for Crocodile Wikipedia Dataset. Uniform sampling has a far shorter runtime than the other methods with comparable accuracy for large samples}{figure.caption.22}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {22}{\ignorespaces Percent Error vs. Runtime for Synthetic Barabási–Albert Dataset. The hybrid approach yields the lowest overall error, but for shorter runtimes, importance sampling performs best.}}{28}{figure.caption.23}\protected@file@percent }
\newlabel{fig:ba_runtime}{{22}{28}{Percent Error vs. Runtime for Synthetic Barabási–Albert Dataset. The hybrid approach yields the lowest overall error, but for shorter runtimes, importance sampling performs best}{figure.caption.23}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {23}{\ignorespaces Percent Error vs. Sample Size for Facebook Dataset. Importance sampling and the hybrid approach perform best, with the hybrid method performing slightly better.}}{29}{figure.caption.24}\protected@file@percent }
\newlabel{fig:fb_sample_size}{{23}{29}{Percent Error vs. Sample Size for Facebook Dataset. Importance sampling and the hybrid approach perform best, with the hybrid method performing slightly better}{figure.caption.24}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {24}{\ignorespaces Percent Error vs. Sample Size for Collaboration Network Datase. Importance sampling and the hybrid approach perform best.}}{29}{figure.caption.25}\protected@file@percent }
\newlabel{fig:grqc_sample_size}{{24}{29}{Percent Error vs. Sample Size for Collaboration Network Datase. Importance sampling and the hybrid approach perform best}{figure.caption.25}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {25}{\ignorespaces Percent Error vs. Sample Size for Crocodile Wikipedia Dataset. The hybrid method performs best.}}{29}{figure.caption.26}\protected@file@percent }
\newlabel{fig:croc_sample_size}{{25}{29}{Percent Error vs. Sample Size for Crocodile Wikipedia Dataset. The hybrid method performs best}{figure.caption.26}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {26}{\ignorespaces Percent Error vs. Sample Size for Synthetic Barabási–Albert Dataset. Importance sampling, variance reduction, and the hybrid method perform best, with importance sampling and the hybrid method performing best.}}{30}{figure.caption.27}\protected@file@percent }
\newlabel{fig:ba_sample_size}{{26}{30}{Percent Error vs. Sample Size for Synthetic Barabási–Albert Dataset. Importance sampling, variance reduction, and the hybrid method perform best, with importance sampling and the hybrid method performing best}{figure.caption.27}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}4-Clique Counting}{30}{subsection.5.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {27}{\ignorespaces Comparison of percent error versus runtime for different methods in 4-clique counting. Importance sampling with higher powers outperforms variance reduction.}}{30}{figure.caption.28}\protected@file@percent }
\newlabel{fig:4_clique_percent_error_runtime_comparison}{{27}{30}{Comparison of percent error versus runtime for different methods in 4-clique counting. Importance sampling with higher powers outperforms variance reduction}{figure.caption.28}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {28}{\ignorespaces Comparison of percent error versus sample size for different 4-clique counting methods. Importance sampling with higher powers outperforms variance reduction.}}{31}{figure.caption.29}\protected@file@percent }
\newlabel{fig:4_clique_percent_error_sample_size_comparison}{{28}{31}{Comparison of percent error versus sample size for different 4-clique counting methods. Importance sampling with higher powers outperforms variance reduction}{figure.caption.29}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {29}{\ignorespaces Average duration of the importance sampling method across powers for 4-clique counting. Runtime increases with sample size and power.}}{31}{figure.caption.30}\protected@file@percent }
\newlabel{fig:4_clique_avg_duration_importance_sampling}{{29}{31}{Average duration of the importance sampling method across powers for 4-clique counting. Runtime increases with sample size and power}{figure.caption.30}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {30}{\ignorespaces Average error of the importance sampling method across powers for 4-clique counting. Average error decreases as sample size increases. The power of 1.5 performs best.}}{31}{figure.caption.31}\protected@file@percent }
\newlabel{fig:4_clique_avg_error_importance_sampling}{{30}{31}{Average error of the importance sampling method across powers for 4-clique counting. Average error decreases as sample size increases. The power of 1.5 performs best}{figure.caption.31}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Simulated Method Tests}{32}{subsection.5.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.3.1}Gaussian/Uniform Noise Relationship}{32}{subsubsection.5.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {31}{\ignorespaces Sample size vs. average error with Gaussian noise (scale = 10). Variance reduction outperforms importance sampling.}}{32}{figure.caption.32}\protected@file@percent }
\newlabel{fig:gaussian_noise_10}{{31}{32}{Sample size vs. average error with Gaussian noise (scale = 10). Variance reduction outperforms importance sampling}{figure.caption.32}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {32}{\ignorespaces Sample size vs. average error with Gaussian noise (scale = 100). Variance reduction outperforms importance sampling.}}{33}{figure.caption.33}\protected@file@percent }
\newlabel{fig:gaussian_noise_100}{{32}{33}{Sample size vs. average error with Gaussian noise (scale = 100). Variance reduction outperforms importance sampling}{figure.caption.33}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {33}{\ignorespaces Sample size vs. average error with Gaussian noise (scale = 500). Variance reduction outperforms importance sampling.}}{33}{figure.caption.34}\protected@file@percent }
\newlabel{fig:gaussian_noise_500}{{33}{33}{Sample size vs. average error with Gaussian noise (scale = 500). Variance reduction outperforms importance sampling}{figure.caption.34}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.3.2}Multiplicative Noise Relationship}{33}{subsubsection.5.3.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {34}{\ignorespaces Sample size vs. average error with multiplicative noise (scale = 0.01). Importance sampling outperforms variance reduction.}}{34}{figure.caption.35}\protected@file@percent }
\newlabel{fig:multiplicative_noise_001}{{34}{34}{Sample size vs. average error with multiplicative noise (scale = 0.01). Importance sampling outperforms variance reduction}{figure.caption.35}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {35}{\ignorespaces Sample size vs. average error with multiplicative noise (scale = 0.1). Importance sampling outperforms variance reduction.}}{34}{figure.caption.36}\protected@file@percent }
\newlabel{fig:multiplicative_noise_01}{{35}{34}{Sample size vs. average error with multiplicative noise (scale = 0.1). Importance sampling outperforms variance reduction}{figure.caption.36}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {36}{\ignorespaces Sample size vs. average error with multiplicative noise (scale = 0.5). Importance sampling outperforms variance reduction.}}{35}{figure.caption.37}\protected@file@percent }
\newlabel{fig:multiplicative_noise_05}{{36}{35}{Sample size vs. average error with multiplicative noise (scale = 0.5). Importance sampling outperforms variance reduction}{figure.caption.37}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Discussion}{36}{section.6}\protected@file@percent }
\newlabel{fig:tri_vs_noise_fb}{{37a}{36}{Facebook}{figure.caption.38}{}}
\newlabel{sub@fig:tri_vs_noise_fb}{{a}{36}{Facebook}{figure.caption.38}{}}
\newlabel{fig:tri_vs_noise_croc}{{37b}{36}{Wikipedia}{figure.caption.38}{}}
\newlabel{sub@fig:tri_vs_noise_croc}{{b}{36}{Wikipedia}{figure.caption.38}{}}
\newlabel{fig:tri_vs_noise_GrQc}{{37c}{36}{Collaboration}{figure.caption.38}{}}
\newlabel{sub@fig:tri_vs_noise_GrQc}{{c}{36}{Collaboration}{figure.caption.38}{}}
\newlabel{fig:tri_vs_noise_ba}{{37d}{36}{Barabási–Albert}{figure.caption.38}{}}
\newlabel{sub@fig:tri_vs_noise_ba}{{d}{36}{Barabási–Albert}{figure.caption.38}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {37}{\ignorespaces Triangle counts vs. noise across all datasets. Nodes with higher triangle counts often have higher noise.}}{36}{figure.caption.38}\protected@file@percent }
\newlabel{fig:tri_vs_noise}{{37}{36}{Triangle counts vs. noise across all datasets. Nodes with higher triangle counts often have higher noise}{figure.caption.38}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Theoretical Analysis of Variances}{36}{subsection.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.1.1}Uniform Sampling}{36}{subsubsection.6.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.1.2}Variance Reduction}{40}{subsubsection.6.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.1.3}Comparing Variance Reduciton and Importance Sampling}{43}{subsubsection.6.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7}Conclusion}{45}{section.7}\protected@file@percent }
\bibstyle{plain}
\bibdata{thesis_bib}
\bibcite{al_hasan_triangle_2018}{1}
\bibcite{albert_statistical_2002}{2}
\bibcite{arifuzzaman_parallel_2012}{3}
\bibcite{avron_counting_2010}{4}
\bibcite{beum_method_1950}{5}
\bibcite{boldrin_fast_2024}{6}
\bibcite{cullum_lanczos_2002}{7}
\bibcite{dinitz_faster_2021}{8}
\bibcite{duan_linear-time_2014}{9}
\bibcite{hutchinson_stochastic_1990}{10}
\bibcite{lovasz_large_2012}{11}
\bibcite{roughgarden_algorithms_2020}{12}
\bibcite{motwani_randomized_1995}{13}
\bibcite{prescott_monte_1965}{14}
\bibcite{seshadhri_triadic_2013}{15}
\bibcite{strassen_gaussian_1969}{16}
\bibcite{su_effect_2016}{17}
\bibcite{tsourakakis_fast_2008}{18}
\bibcite{tsourakakis_doulion_2009}{19}
\bibcite{williams_new_2023}{20}
\bibcite{ye_commensurate_2005}{21}
\gdef \@abspage@last{47}
